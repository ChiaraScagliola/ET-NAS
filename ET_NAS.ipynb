{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIsGjTfcbv-y",
        "outputId": "1a5dc4f6-eff0-4090-94ea-4901ef0ad04b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybZcbZlBclww",
        "outputId": "c4cf8f07-7129-433c-de03-60dbb80fa5c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/GitHub\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tgj8z3YjcqsB"
      },
      "outputs": [],
      "source": [
        "repository = \"NAS_project\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCjRhZdwcrs4",
        "outputId": "9294988d-40aa-41e7-dae9-0315547a0d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/GitHub/NAS_project\n"
          ]
        }
      ],
      "source": [
        "%cd {repository}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pW1eAFrczUX"
      },
      "outputs": [],
      "source": [
        "pip install nats_bench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpSGj_ccc4KI"
      },
      "outputs": [],
      "source": [
        "pip install yacs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW5Pmym0c7TR"
      },
      "outputs": [],
      "source": [
        "pip install simplejson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4BRbwA8c-aF"
      },
      "outputs": [],
      "source": [
        "pip install xautodl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQHmOqI_iJId"
      },
      "outputs": [],
      "source": [
        "pip install pytorchcv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEheGLe8dGRK",
        "outputId": "a69c1105-51ca-4c18-bb13-b500263e3ab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: TORCH_HOME=/content/drive/MyDrive/NAS-PULITO/.torch\n"
          ]
        }
      ],
      "source": [
        "%env TORCH_HOME=/content/drive/MyDrive/NAS-PULITO/.torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# import and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "l6JAXjXEdHYS"
      },
      "outputs": [],
      "source": [
        "# import librerie\n",
        "\n",
        "# librerie giÃ  presenti nelle prime versioni\n",
        "import torch\n",
        "import argparse\n",
        "import datasets \n",
        "import nasspace\n",
        "import pandas as pd\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "from timeit import default_timer as timer\n",
        "import os\n",
        "from scipy import stats\n",
        "from nats_bench import create\n",
        "import xautodl  # import this lib -- \"https://github.com/D-X-Y/AutoDL-Projects\", you can use pip install xautodl\n",
        "from xautodl.models import get_cell_based_tiny_net\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from numpy import inf\n",
        "\n",
        "#librerie nuove\n",
        "from pycls.models.nas.nas import Cell\n",
        "import pandas as pd\n",
        "from pytorchcv.model_provider import get_model as ptcv_get_model\n",
        "from pytorchcv.model_provider import _models as ptcv_models\n",
        "from pruners import *\n",
        "from pruners import predictive\n",
        "from tqdm import trange\n",
        "from statistics import mean\n",
        "from sklearn.preprocessing import normalize\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "from GenNAS.builder_task import *\n",
        "from GenNAS.builder_model import *\n",
        "from GenNAS.builder_evaluator import *\n",
        "from GenNAS.utils.config_generator import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "p3VW6r7EYRxp"
      },
      "outputs": [],
      "source": [
        "#SELECT DATASET\n",
        "#Dataset_name = \"Cifar-10\"\n",
        "#Dataset_name = \"Cifar-100\"\n",
        "Dataset_name = \"ImageNet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0t7HP6w6eD2q"
      },
      "outputs": [],
      "source": [
        "GPU = '0'\n",
        "seed = 1\n",
        "batch_size = 128\n",
        "\n",
        "#CIFAR10\n",
        "if Dataset_name == \"Cifar-10\":\n",
        "  dataset = 'cifar10'\n",
        "  data_loc = 'insert path cifar-10'\n",
        "\n",
        "\n",
        "#CIFAR100\n",
        "if Dataset_name == \"Cifar-100\":\n",
        "  dataset = 'cifar100'\n",
        "  data_loc = \"insert path cifar-100\"\n",
        "\n",
        "#IMAGENET\n",
        "if Dataset_name == \"ImageNet\":\n",
        "  dataset = 'ImageNet16-120'\n",
        "  data_loc = \"insert path ImageNet16\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7bKx87xQel8n"
      },
      "outputs": [],
      "source": [
        "os.environ['CUDA_VISIBLE_DEVICES'] = GPU\n",
        "\n",
        "# Reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rZTei2yNfFZv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def get_naswot(network, x):\n",
        "  network = network.to(device)\n",
        "\n",
        "  def boolRelu_forward_hook(module, inp, out):\n",
        "    if isinstance(out, tuple):\n",
        "      out = out[0]\n",
        "    out = out.view(out.size(0), -1)   \n",
        "    x = (out > 0).float()  \n",
        "    network.boolRelu = torch.cat((network.boolRelu, x), -1)\n",
        "    \n",
        "  network.boolRelu = torch.tensor([])\n",
        "  network.boolRelu = network.boolRelu.to(device)\n",
        "\n",
        "  for name, module in network.named_modules(): \n",
        "    if (isinstance(module, torch.nn.modules.activation.ReLU)):\n",
        "      module.register_forward_hook(boolRelu_forward_hook)\n",
        "              \n",
        "\n",
        "  network(x)\n",
        "  k = (network.boolRelu @ network.boolRelu.t()) + ((1. - network.boolRelu) @ (1. - network.boolRelu.t()))\n",
        "  logdet = torch.linalg.slogdet(k)[1].cpu().detach()\n",
        "  torch.cuda.empty_cache()\n",
        "  return logdet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QpShe1zKff-2"
      },
      "outputs": [],
      "source": [
        "def NNDegree(searchspace, uid):\n",
        "\n",
        "  strOp = searchspace.get_str(uid)\n",
        "  listOp = strOp.split('|')\n",
        "  nConv = strOp.count('conv')\n",
        "  skip1 = [3,7]\n",
        "  skip2 = [6]\n",
        "  nS1 = ''.join([i for j, i in enumerate(listOp) if j in skip1]).count('skip')#skip 1 layer\n",
        "  nS2  = ''.join([i for j, i in enumerate(listOp) if j in skip2]).count('skip')#skip 2 layers\n",
        "  Sc = (nS1+2*nS2) #total skipped layers\n",
        "  Wc = nConv\n",
        "  return int((Wc + Sc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "h7cSA1fNftMi"
      },
      "outputs": [],
      "source": [
        "def get_uid_and_measure(uid, input, metric_name, searchspace, population = None):\n",
        "  scaler = StandardScaler()\n",
        "  uid = int(uid)\n",
        "  network = searchspace.get_network(uid)\n",
        "  network.to(device)\n",
        "  if metric_name == \"Naswot\":\n",
        "    return uid, get_naswot(network, input).item()\n",
        "  elif metric_name == \"log_synflow\":\n",
        "    deg =  NNDegree(searchspace, uid)\n",
        "    return uid, predictive.get_log_syn(network, input), deg\n",
        "  else:\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1Fe9Ab7Xb_pr"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def sum_syn_naswot_degree(list_measures, input, searchspace, task, model_builder, evaluator):\n",
        "  nas = []\n",
        "  slope = []\n",
        "  scaler = StandardScaler()\n",
        "  list_measures[list_measures == -inf] = 0\n",
        "  syn = list_measures[:,1].reshape(-1,1)\n",
        "  syn_nor = scaler.fit_transform(list_measures[:,1].reshape(-1,1))\n",
        "  degree = list_measures[:,2].reshape(-1,1)\n",
        "  deg = scaler.fit_transform(list_measures[:,2].reshape(-1,1))\n",
        "  for uid in list_measures[:,0]:\n",
        "      network = searchspace.get_network(int(uid))\n",
        "      network.to(device)\n",
        "      nas.append([ get_naswot(network, input).item()])\n",
        "      \n",
        "  naswot = np.array(nas).reshape(-1,1)\n",
        "  WNor = scaler.fit_transform(np.array(nas).reshape(-1,1))\n",
        "  measures = np.hstack((list_measures[:,0].reshape(-1,1), syn_nor+WNor+deg, syn, naswot, degree)) \n",
        "  return measures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BKWeqA9vf-r1"
      },
      "outputs": [],
      "source": [
        "def get_uid_and_measures(uid_list, input, searchspace, metric_name):\n",
        "  naswot = []\n",
        "  log_Syn = []\n",
        "  norm = []\n",
        "\n",
        "  if metric_name == \"Naswot\":\n",
        "    for uid in uid_list:\n",
        "      network = searchspace.get_network(int(uid))\n",
        "      network.to(device)\n",
        "      naswot.append([uid, get_naswot(network, input).item()])\n",
        "    return np.array(naswot)\n",
        "  elif metric_name == \"log_synflow\":\n",
        "    for uid in uid_list:\n",
        "      deg =  NNDegree(searchspace, uid)\n",
        "      network = searchspace.get_network(int(uid))\n",
        "      log_Syn.append([uid, predictive.get_log_syn(network, input), deg])\n",
        "    return np.array(log_Syn)\n",
        "  else:\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MA2PRbOigFF7"
      },
      "outputs": [],
      "source": [
        "def get_archs(searchspace,list_uid):\n",
        "  list_arch = []\n",
        "  for uid in list_uid:\n",
        "    list_arch.append(searchspace.get_unique_str(uid))\n",
        "  return list_arch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "toRXYQR2gF2p"
      },
      "outputs": [],
      "source": [
        "def lossSlope(losslist):\n",
        "  trend = np.polyfit(range(len(losslist[1:10])),losslist[1:10],1)\n",
        "  return trend[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "eqD0BGNSgN6i"
      },
      "outputs": [],
      "source": [
        "def get_regression_score(uid, task, model_builder, evaluator, searchspace):\n",
        "  arch = searchspace.get_str(uid)\n",
        "  losses = evaluator.evaluate(task, model_builder, arch)\n",
        "  return -lossSlope(list(losses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Q3er95CXco8o"
      },
      "outputs": [],
      "source": [
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def dominates(obj1, obj2, sign=[1, 1]):\n",
        "    indicator = False\n",
        "    for a, b, sign in zip(obj1, obj2, sign):\n",
        "        if a * sign > b * sign:\n",
        "            indicator = True\n",
        "        # if one of the objectives is dominated, then return False\n",
        "        elif a * sign < b * sign:\n",
        "            return False\n",
        "    return indicator\n",
        "\n",
        "\n",
        "def sortNondominated(fitness, k=None, first_front_only=False):\n",
        "   \n",
        "    fitness = np.ndarray.tolist(fitness)\n",
        "    if k is None:\n",
        "        k = len(fitness)\n",
        "\n",
        "    # Use objectives as keys to make python dictionary\n",
        "    map_fit_ind = defaultdict(list)\n",
        "    for i, f_value in enumerate(fitness):  # fitness = [(1, 2), (2, 2), (3, 1), (1, 4), (1, 1)...]\n",
        "        map_fit_ind[tuple(f_value)].append(i)\n",
        "    fits = list(map_fit_ind.keys())  # fitness values\n",
        "\n",
        "    current_front = []\n",
        "    next_front = []\n",
        "    dominating_fits = defaultdict(int)  # n (The number of people dominate you)\n",
        "    dominated_fits = defaultdict(list)  # Sp (The people you dominate)\n",
        "\n",
        "    # Rank first Pareto front\n",
        "    # *fits* is a iterable list of chromosomes. Each has multiple objectives.\n",
        "    for i, fit_i in enumerate(fits):\n",
        "        for fit_j in fits[i + 1:]:\n",
        "            # Eventhougn equals or empty list, n & Sp won't be affected\n",
        "            if dominates(fit_i, fit_j):\n",
        "                dominating_fits[fit_j] += 1  \n",
        "                dominated_fits[fit_i].append(fit_j)  \n",
        "            elif dominates(fit_j, fit_i):  \n",
        "                dominating_fits[fit_i] += 1\n",
        "                dominated_fits[fit_j].append(fit_i)\n",
        "        if dominating_fits[fit_i] == 0: \n",
        "            current_front.append(fit_i)\n",
        "\n",
        "    fronts = [[]]  # The first front\n",
        "    for fit in current_front:\n",
        "        fronts[-1].extend(map_fit_ind[fit])\n",
        "    pareto_sorted = len(fronts[-1])\n",
        "\n",
        "    # Rank the next front until all individuals are sorted or\n",
        "    # the given number of individual are sorted.\n",
        "    # If Sn=0 then the set of objectives belongs to the next front\n",
        "    if not first_front_only:  # first front only\n",
        "        N = min(len(fitness), k)\n",
        "        while pareto_sorted < N:\n",
        "            fronts.append([])\n",
        "            for fit_p in current_front:\n",
        "                # Iterate Sn in current fronts\n",
        "                for fit_d in dominated_fits[fit_p]: \n",
        "                    dominating_fits[fit_d] -= 1  # Next front -> Sn - 1\n",
        "                    if dominating_fits[fit_d] == 0:  # Sn=0 -> next front\n",
        "                        next_front.append(fit_d)\n",
        "                         # Count and append chromosomes with same objectives\n",
        "                        pareto_sorted += len(map_fit_ind[fit_d]) \n",
        "                        fronts[-1].extend(map_fit_ind[fit_d])\n",
        "            current_front = next_front\n",
        "            next_front = []\n",
        "\n",
        "    return np.array(fronts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "O9j7w2I6UP5U"
      },
      "outputs": [],
      "source": [
        "def closest_node(node, nodes):\n",
        "    nodes = np.asarray(nodes)\n",
        "    dist_2 = np.sum((nodes - node)**2, axis=1)\n",
        "    return np.argmin(dist_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "9-3gVvRpgQ4K"
      },
      "outputs": [],
      "source": [
        "class args:\n",
        "  pass\n",
        "\n",
        "args.total_iters = 10 #100\n",
        "args.eval_interval = 1 #10\n",
        "args.init_w_type = 'none'\n",
        "args.init_b_type = 'none'\n",
        "args.learning_rate = 1e-1\n",
        "args.weight_decay = 4e-5\n",
        "args.momentum = 0.9 \n",
        "args.eval_weights = [0.25,0.5,1.]\n",
        "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "args.train_weights = [0.25,0.5,1.]\n",
        "args.config = \"CONF_NB101\"\n",
        "args.workers = 2\n",
        "args.pad = 'store_true'\n",
        "args.input_size = 32 #cifar 32?\n",
        "args.output_size = 8 #cifar 8?\n",
        "args.last_channels = 64\n",
        "args.batch_size = 16 #16\n",
        "#CIFAR10\n",
        "if Dataset_name == \"Cifar-10\":\n",
        "  args.data_loc = 'insert path cifar-10'\n",
        "  args.dataset = 'cifar10'\n",
        "  args.num_labels = 10\n",
        "\n",
        "#CIFAR100\n",
        "if Dataset_name == \"Cifar-100\":\n",
        "  args.dataset = 'cifar100'\n",
        "  args.data_loc = \"insert path cifar-100\"\n",
        "  args.num_labels = 100\n",
        "\n",
        "#IMAGENET\n",
        "if Dataset_name == \"ImageNet\":\n",
        "  args.dataset = 'ImageNet16-120'\n",
        "  args.data_loc = \"insert path ImageNet16\"\n",
        "  args.num_labels = 120\n",
        "\n",
        "args.init_channels = 16\n",
        "args.search_space = 'nasbench201'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_f3Sg6RDg3KP"
      },
      "outputs": [],
      "source": [
        "args.config = eval(args.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "D6mMlZzsg37v"
      },
      "outputs": [],
      "source": [
        "evaluator = Evaluator(args)\n",
        "task = CVTask(args)\n",
        "model_builder = ModelBuilder(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6Ad9Tir0x4ht"
      },
      "outputs": [],
      "source": [
        "df_config = pd.read_csv(\"conf.csv\") #dataframe of networks configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3gbWoWVx8tk",
        "outputId": "99984de2-0b35-4dc3-ebc9-becb07649f8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-08-28 19:17:02] Try to use the default NATS-Bench (topology) path from fast_mode=True and path=None.\n"
          ]
        }
      ],
      "source": [
        "searchspace = nasspace.get_search_space(dataset, df_config.conf)\n",
        "train_loader = datasets.get_data(dataset, data_loc, batch_size)\n",
        "\n",
        "data_iterator = iter(train_loader)\n",
        "input, target = next(data_iterator)\n",
        "input, target = input.to(device), target.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh5OWISoJFRz"
      },
      "source": [
        "# ET-NAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkYyB79BimiO"
      },
      "outputs": [],
      "source": [
        "from numpy.random import default_rng\n",
        "\n",
        "Number_experiments = 30\n",
        "filename = f'ET-NAS_on_{dataset}_in_{Number_experiments}_experimets.csv'\n",
        "arch_dict = []\n",
        "\n",
        "arch_info = ['arch_id' , 'time', 'acc', 'test_acc']\n",
        "history = []\n",
        "from numpy.random import default_rng\n",
        "population_size=20 \n",
        "n_sample=5\n",
        "cycles= 60\n",
        "\n",
        "for j in range(0,Number_experiments):\n",
        "  \n",
        "  new_gen = []\n",
        "  history = []\n",
        "\n",
        "\n",
        "  start = timer()\n",
        "\n",
        "  uid_population =  random.sample(range(len(searchspace)),population_size)\n",
        "\n",
        "  population =  get_uid_and_measures(uid_population, input, searchspace, \"log_synflow\") \n",
        "\n",
        "  history = get_archs(searchspace, uid_population)\n",
        "\n",
        "          \n",
        "  while len(history) <= cycles:\n",
        "\n",
        "      uid_sample = random.sample(range(len(population)),n_sample)\n",
        "\n",
        "      sample = population[uid_sample]\n",
        "\n",
        "      parent = max(sample[sample[:,2] == max(sample[:,2])], key=lambda i: i[1]) \n",
        "\n",
        "      child_idx = searchspace.mutate_arch(int(parent[0])) \n",
        "\n",
        "      unique_str_child = searchspace.get_unique_str(child_idx)\n",
        "\n",
        "      degree_child = NNDegree(searchspace, child_idx)\n",
        "\n",
        "      degree_parent = parent[-1]\n",
        "\n",
        "      if unique_str_child not in history and degree_child >= degree_parent:\n",
        "\n",
        "\n",
        "        uid_and_measure = np.array(get_uid_and_measure(child_idx, input, \"log_synflow\", searchspace, population))\n",
        "\n",
        "        new_gen.append([int(uid_and_measure[0]), uid_and_measure[1], uid_and_measure[2] ])\n",
        "\n",
        "        population = np.delete(population, 0, axis=0)\n",
        "\n",
        "        population = np.vstack((population, uid_and_measure))\n",
        "\n",
        "        history.append(unique_str_child)\n",
        "\n",
        "\n",
        "     \n",
        "          \n",
        "          \n",
        "  \n",
        "\n",
        "  gen = np.array(new_gen)\n",
        "\n",
        "  gen = gen[gen[:,2] == max(gen[:,2])] #consider the architectures with higher NNdegree\n",
        "\n",
        "  measure= sum_syn_naswot_degree(gen, input, searchspace,task,model_builder,evaluator) \n",
        "\n",
        "  syn_naswot = measure[:,[2,3]]\n",
        "\n",
        "  pareto_index = sortNondominated(syn_naswot, first_front_only=True)\n",
        "  syn_naswot_pareto = syn_naswot[pareto_index[0]]\n",
        "  arch_pareto = measure[pareto_index[0]][:,0]\n",
        "\n",
        "\n",
        "  regression = []\n",
        "\n",
        "\n",
        "  if len(arch_pareto) > 1:\n",
        "    for uid in arch_pareto:\n",
        "      regression.append(get_regression_score(uid, task, model_builder, evaluator, searchspace))\n",
        "    \n",
        "    syn_naswot_regression = np.hstack((syn_naswot_pareto, np.array(regression).reshape(-1,1)))\n",
        "    measures_normalized = (syn_naswot_regression) / (syn_naswot_regression.max(axis = 0))\n",
        "    best_arch_index = closest_node(np.array([1,1,1]), measures_normalized)\n",
        "    best_arch = arch_pareto[best_arch_index]\n",
        "    total_time_cost = (timer()-start)\n",
        "\n",
        "  else:\n",
        "    best_arch = arch_pareto[0]\n",
        "    total_time_cost = (timer()-start)\n",
        "\n",
        "\n",
        "  arch_dict.append({'arch_id' : int(best_arch), 'time': total_time_cost, 'test_acc': searchspace.get_final_accuracy(int(best_arch)) })\n",
        "\n",
        "  with open(filename, 'w') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames = arch_info)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(arch_dict)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z2U1czQWvXL",
        "outputId": "3aeb8b5b-3e8e-4001-f407-0790eb0a565b"
      },
      "outputs": [],
      "source": [
        "OurRea_on_ImageNet16_120 = pd.read_csv(\"ET-NAS_on_dataset_in_Number_experiments_experimets.csv\")#insert path of csv output\n",
        "print(OurRea_on_ImageNet16_120.mean())\n",
        "print(OurRea_on_ImageNet16_120.std())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ET-NAS.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "78df7d939e47aa6a0bf5eb7a9dc5a437fa0ea5f2f01dfe65549b914418145645"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
